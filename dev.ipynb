{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41083305",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70bad916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.10.7)\n",
      "Requirement already satisfied: pandas in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.7.2)\n",
      "Requirement already satisfied: nltk in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.9.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.60.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.2.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (12.0.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hitomi/.local/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hitomi/.local/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hitomi/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hitomi/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/hitomi/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.15.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hitomi/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in /home/hitomi/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: click in /home/hitomi/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (8.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# install required libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd86ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.data import find\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import MLEProbDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('tagsets_json')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger model\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # POS tagger model\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "POS_TAGS = tagdict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc48471",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6529b126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data_folder_name = 'A4data_2526_pan2020'\n",
    "DATA_FOLDER = Path(data_folder_name)\n",
    "DEV_SET = None\n",
    "TEST_SET = None\n",
    "TRAIN_SET = None\n",
    "\n",
    "if not DATA_FOLDER.exists():\n",
    "    print(f\"ERROR: Data folder not found, make sure the '/{data_folder_name}' folder is located in the same folder as this notebook!\")\n",
    "else:\n",
    "    try:\n",
    "        DEV_SET = pd.read_csv(DATA_FOLDER / 'pan2526_dev_data.csv')\n",
    "        TEST_SET = pd.read_csv(DATA_FOLDER / 'pan2526_test_data.csv')\n",
    "        TRAIN_SET = pd.read_csv(DATA_FOLDER / 'pan2526_train_data.csv')\n",
    "        print('Data loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Data not found in '/{DATA_FOLDER}'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d5f85",
   "metadata": {},
   "source": [
    "Let's have a look at the DEV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4074360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3329</td>\n",
       "      <td>The last part of the pictorial will now begin!...</td>\n",
       "      <td>560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1225</td>\n",
       "      <td>\"It is a huge body of water.\" It was slow yet ...</td>\n",
       "      <td>560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3334</td>\n",
       "      <td>As she reached for the door, the sound of a ca...</td>\n",
       "      <td>512464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3573</td>\n",
       "      <td>Despite the scowl that seemed to be forever pl...</td>\n",
       "      <td>2750536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1429</td>\n",
       "      <td>The traditional institutions did not take the ...</td>\n",
       "      <td>1112924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1432</td>\n",
       "      <td>\"Still thinking about it, Kuwabara-jiisan,\" Hi...</td>\n",
       "      <td>1112924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1838</td>\n",
       "      <td>\"Well, if it isn\"t tha\" police kitteh.\" Seras ...</td>\n",
       "      <td>1220273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>3328</td>\n",
       "      <td>\"I think I have seen that clothes before...\" L...</td>\n",
       "      <td>560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>470</td>\n",
       "      <td>\"I almost didn\"t turn away from you, even when...</td>\n",
       "      <td>1220273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2037</td>\n",
       "      <td>\"No...\" \"Alright, so what you want to do is pu...</td>\n",
       "      <td>2855986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               text   author\n",
       "0          3329  The last part of the pictorial will now begin!...   560480\n",
       "1          1225  \"It is a huge body of water.\" It was slow yet ...   560480\n",
       "2          3334  As she reached for the door, the sound of a ca...   512464\n",
       "3          3573  Despite the scowl that seemed to be forever pl...  2750536\n",
       "4          1429  The traditional institutions did not take the ...  1112924\n",
       "..          ...                                                ...      ...\n",
       "263        1432  \"Still thinking about it, Kuwabara-jiisan,\" Hi...  1112924\n",
       "264        1838  \"Well, if it isn\"t tha\" police kitteh.\" Seras ...  1220273\n",
       "265        3328  \"I think I have seen that clothes before...\" L...   560480\n",
       "266         470  \"I almost didn\"t turn away from you, even when...  1220273\n",
       "267        2037  \"No...\" \"Alright, so what you want to do is pu...  2855986\n",
       "\n",
       "[268 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_SET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4a7b8",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb9c479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(sentences):\n",
    "    return [nltk.pos_tag(word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "def prepare(data_frame):\n",
    "    data_frame['snippet_length'] = data_frame['text'].str.len()\n",
    "    \n",
    "    data_frame['tokens'] = data_frame['text'].apply(\n",
    "        lambda x: word_tokenize(x.lower())\n",
    "    )\n",
    "    data_frame['token_count'] = data_frame['tokens'].apply(\n",
    "        lambda x: len(x)\n",
    "    )\n",
    "    data_frame['sentences'] = data_frame['text'].apply(\n",
    "        lambda x: sent_tokenize(x.lower())\n",
    "    )\n",
    "    data_frame['sentence_count'] = data_frame['sentences'].apply(\n",
    "        lambda x: len(x)\n",
    "    )\n",
    "    data_frame['pos_tagged_sentences'] = data_frame['sentences'].apply(\n",
    "        lambda x: pos_tagger(x)\n",
    "    )\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24b32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_counter(pos_tagged_sentences, tag):\n",
    "    tag_sum = 0\n",
    "    for sentence in pos_tagged_sentences:\n",
    "        for token_tag_pair in sentence:\n",
    "            if token_tag_pair[1] == tag:\n",
    "                tag_sum += 1\n",
    "    return tag_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9ba6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_features(data_frame):\n",
    "    chosen_tags = ['CC', 'CD', 'DT', 'EX', 'FW', \n",
    "'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS',\n",
    "'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG',\n",
    "'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "    \n",
    "    \n",
    "    assert 'pos_tagged_sentences' in data_frame.columns\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    for tag in chosen_tags:\n",
    "        feature_name = f'pf_{tag}'\n",
    "    \n",
    "        data_frame[feature_name] = data_frame['pos_tagged_sentences'].apply(\n",
    "            lambda x: tag_counter(x, tag)\n",
    "        ) / data_frame['token_count']\n",
    "\n",
    "    tag_sums = {}\n",
    "    for tag in chosen_tags:\n",
    "        feature_name = f'pf_{tag}'\n",
    "        tag_sums[tag] = data_frame[feature_name].sum()\n",
    "    \n",
    "    top_tags = sorted(tag_sums, key=tag_sums.get, reverse=True)[:10]\n",
    "    print(f'top_keys: {top_tags}')\n",
    "    for tag in top_tags:\n",
    "        feature_name = f'pf_{tag}'\n",
    "        feature_names.append(feature_name)\n",
    "        print(f'{tag}: {data_frame[feature_name].sum()}')\n",
    "    \n",
    "    return data_frame, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0bb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(data_frame):\n",
    "#     data_frame['tokenized'] = data_frame['text'].apply(\n",
    "#         lambda x: word_tokenize(x.lower())\n",
    "#     )\n",
    "#     return data_frame\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf4b1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_features(data_frame):\n",
    "    # some char count fetaures\n",
    "    \n",
    "    data_frame['cf_comma'] = data_frame['text'].str.count(r',') / data_frame['snippet_length']\n",
    "    data_frame['cf_period'] = data_frame['text'].str.count(r'\\.') / data_frame['snippet_length']\n",
    "    data_frame['cf_exclam'] = data_frame['text'].str.count(r'\\!') / data_frame['snippet_length']\n",
    "    data_frame['cf_question'] = data_frame['text'].str.count(r'\\?') / data_frame['snippet_length']\n",
    "    data_frame['cf_upper_case'] = data_frame['text'].str.count(r'[A-Z]') / data_frame['snippet_length']\n",
    "    data_frame['cf_lower_case'] = data_frame['text'].str.count(r'[a-z]') / data_frame['snippet_length']\n",
    "    \n",
    "    # some aggregated char count features\n",
    "    data_frame['cf_vowel_frequency'] = data_frame['text'].str.count(r'[aeiou]') / data_frame['snippet_length']\n",
    "    data_frame['cf_avg_word_len'] = data_frame['text'].apply(lambda x: sum(len(w) for w in x.split()) / len(x.split()) if x.split() else 0)\n",
    "    \n",
    "    # make sure any features you want to be used during training are also in this list:\n",
    "    feature_names = [\n",
    "        'cf_comma', \n",
    "        'cf_period', \n",
    "        'cf_exclam', \n",
    "        'cf_question', \n",
    "        'cf_upper_case',\n",
    "        'cf_lower_case',\n",
    "        'cf_vowel_frequency',\n",
    "        'cf_avg_word_len'\n",
    "    ]\n",
    "    \n",
    "    return data_frame, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1d2c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(data_frame):\n",
    "    # count number of words\n",
    "    data_frame['wf_word_count'] = data_frame['text'].apply(\n",
    "        lambda x: len(x.split())\n",
    "    )\n",
    "    # count the stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data_frame['wf_stopword_frequency'] = data_frame['text'].apply(\n",
    "        lambda x: sum(1 for w in word_tokenize(x.lower()) if w in stop_words)\n",
    "    ) / data_frame['wf_word_count']\n",
    "    \n",
    "    # make sure any features you want to be used during training are also in this list:\n",
    "    feature_names = [\n",
    "        'wf_word_count', \n",
    "        'wf_stopword_frequency'\n",
    "    ]\n",
    "    \n",
    "    return data_frame, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02fca92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_features(data_frame, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        #normalizing the score and extracting only the top 10 words\n",
    "        vectorizer = TfidfVectorizer(max_features=10)\n",
    "        matrix = vectorizer.fit_transform(data_frame['text'])\n",
    "    else:\n",
    "        matrix = vectorizer.transform(data_frame['text'])\n",
    "    feature_names = [\n",
    "        f'Tfidf_{w}' for w in vectorizer.get_feature_names_out()\n",
    "    ]\n",
    "    bow_df = pd.DataFrame(matrix.toarray(), columns=feature_names, index=data_frame.index)\n",
    "    data_frame = pd.concat([data_frame, bow_df], axis=1)\n",
    "    return data_frame, feature_names, vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "168c3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing pairs consisting only stopwords\n",
    "def remove_stopwords_pairs(ngram):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    removed = []\n",
    "    for pair in ngram:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word not in stop_words:\n",
    "                count = 1\n",
    "                break\n",
    "        if (count==1):\n",
    "            removed.append(pair)\n",
    "    return removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e02698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_topnth_prob(data_frame, n):\n",
    "    topn_list = []\n",
    "    for token in data_frame['tokens']:\n",
    "        bigram_list = list(bigrams(token))\n",
    "        bigram_stopwords_removed_list = remove_stopwords_pairs(bigram_list)\n",
    "    \n",
    "        if not bigram_stopwords_removed_list:\n",
    "            topn_list.append({})\n",
    "            continue\n",
    "\n",
    "        freq_dist = FreqDist(bigram_stopwords_removed_list)\n",
    "        prob_dist = MLEProbDist(freq_dist)\n",
    "\n",
    "        bigram_prob_dict = {bg: prob_dist.prob(bg) for bg in freq_dist.keys()}\n",
    "\n",
    "        sorted_dict = dict(sorted(bigram_prob_dict.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "        topn_list.append(sorted_dict)\n",
    "\n",
    "    \n",
    "    bigram_prob_df = pd.DataFrame(topn_list).fillna(0)\n",
    "    bigram_prob_df.columns = [f'bigram_{bg}' for bg in bigram_prob_df.columns]\n",
    "\n",
    "    data_frame = pd.concat([data_frame.reset_index(drop=True), bigram_prob_df.reset_index(drop=True)], axis=1)\n",
    "    feature_names = list(bigram_prob_df.columns)\n",
    "    return data_frame, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8be30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_frame):\n",
    "    \"\"\"extracts relevant features\"\"\"\n",
    "    # prepare text (like tokenizing)\n",
    "    data_frame = prepare(data_frame)\n",
    "    print(data_frame.head())\n",
    "    # list of feature names we want to use for training\n",
    "    feature_names = []\n",
    "    feature_groups = {}\n",
    "    \n",
    "    # extract some character based features\n",
    "    data_frame, ft_names = get_char_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"char\"] = ft_names\n",
    "    # extract stop word based features\n",
    "    data_frame, ft_names = get_word_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"stopword_frequency\"] = ft_names\n",
    "    \n",
    "    # extract POS features\n",
    "    data_frame, ft_names = get_pos_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"pos_tags\"] = ft_names\n",
    "\n",
    "    #extract bag of words feature using the top 10 words\n",
    "    data_frame, ft_names, vectorizer = get_bow_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"Tfidf\"] = ft_names\n",
    "    \n",
    "    data_frame, ft_names = get_bigram_topnth_prob(data_frame, 50)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"bigrams\"] = ft_names\n",
    "\n",
    "    # return the data with the features and the list of feature names\n",
    "    return data_frame, feature_names, feature_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd813084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    A wrapper class that is meant to wrap around an arbitrary machine learning model\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, feature_names, *args, verbose=False, **kwargs):\n",
    "        self.args=args\n",
    "        self.kwargs=kwargs\n",
    "        self.X_train=X_train[feature_names]\n",
    "        self.y_train=y_train\n",
    "        self.feature_names=feature_names\n",
    "        self.verbose=verbose\n",
    "    \n",
    "    def fit(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _test(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _score_fn(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X_test, y_test):\n",
    "        self.X_test=X_test[self.feature_names]\n",
    "        self.y_test=y_test\n",
    "        return self._test()\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        self.X_test=X_test[self.feature_names]\n",
    "        self.y_test=y_test\n",
    "        return self._score_fn()\n",
    "    \n",
    "    @classmethod\n",
    "    def train_and_score(cls, X_train, y_train, X_test, y_test, feature_names, *args, verbose=False, **kwargs):\n",
    "        model = cls(X_train, y_train, feature_names, *args, verbose=verbose, **kwargs)\n",
    "        model.fit()\n",
    "        return model.score(X_test, y_test)\n",
    "    \n",
    "\n",
    "class DecisionTree(Model):\n",
    "    \"\"\"\n",
    "    Wrapper for the sklearn DecisionTree\n",
    "    \"\"\"\n",
    "    def fit(self):\n",
    "        self.model = DecisionTreeClassifier(*self.args, **self.kwargs)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def _test(self):\n",
    "        # Make predictions\n",
    "        return self.model.predict(self.X_test)\n",
    "    \n",
    "    def _score_fn(self):\n",
    "        return self.model.score(self.X_test, self.y_test)\n",
    "    \n",
    "class RandomForest(Model):\n",
    "    \"\"\"\n",
    "    Wrapper for the sklearn RandomForest\n",
    "    \"\"\"\n",
    "    def fit(self):\n",
    "        self.model = RandomForestClassifier(*self.args, **self.kwargs)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def _test(self):\n",
    "        # Make predictions for the test set\n",
    "        return self.model.predict(self.X_test)\n",
    "    \n",
    "    def _score_fn(self):\n",
    "        # Get the accuracy on the test set\n",
    "        return self.model.score(self.X_test, self.y_test)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0c2471e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_test(model_type:Model, data_frame, feature_names, *args, **kwargs):\n",
    "    X = data_frame[feature_names]\n",
    "    y = data_frame['author']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scores = {}\n",
    "    # get the base score when using all features:\n",
    "    scores['None'] = model_type.train_and_score(\n",
    "        *args,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        feature_names=feature_names,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # perform the acual ablation tests:\n",
    "    for feature in feature_names:\n",
    "        # copy the feature list and remove one:\n",
    "        ablated_feature_names = feature_names.copy() \n",
    "        ablated_feature_names.remove(feature)  \n",
    "        \n",
    "        # for nicer printing in the plot func:\n",
    "        prettified_feature_name = feature.replace(\"_\", \" \")  \n",
    "        \n",
    "        # train and test the model\n",
    "        scores[prettified_feature_name] = model_type.train_and_score(\n",
    "            *args,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            feature_names=ablated_feature_names,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "    fig, ax = plt.subplots(1,1, figsize=(12,12))\n",
    "    bars = ax.bar(scores.keys(), scores.values())\n",
    "    bars[0].set_color('C1')\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_rotation(45)\n",
    "        label.set_ha('right')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Ablated feature')\n",
    "    ax.set_title(f'Ablation test ({model_type.__name__})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8ff998d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text   author  \\\n",
      "0        3329  The last part of the pictorial will now begin!...   560480   \n",
      "1        1225  \"It is a huge body of water.\" It was slow yet ...   560480   \n",
      "2        3334  As she reached for the door, the sound of a ca...   512464   \n",
      "3        3573  Despite the scowl that seemed to be forever pl...  2750536   \n",
      "4        1429  The traditional institutions did not take the ...  1112924   \n",
      "\n",
      "   snippet_length                                             tokens  \\\n",
      "0            3094  [the, last, part, of, the, pictorial, will, no...   \n",
      "1            3072  [``, it, is, a, huge, body, of, water, ., '', ...   \n",
      "2            2878  [as, she, reached, for, the, door, ,, the, sou...   \n",
      "3            3095  [despite, the, scowl, that, seemed, to, be, fo...   \n",
      "4            3044  [the, traditional, institutions, did, not, tak...   \n",
      "\n",
      "   token_count                                          sentences  \\\n",
      "0          749  [the last part of the pictorial will now begin...   \n",
      "1          697  [\"it is a huge body of water.\", it was slow ye...   \n",
      "2          737  [as she reached for the door, the sound of a c...   \n",
      "3          711  [despite the scowl that seemed to be forever p...   \n",
      "4          669  [the traditional institutions did not take the...   \n",
      "\n",
      "   sentence_count                               pos_tagged_sentences  \\\n",
      "0              49  [[(the, DT), (last, JJ), (part, NN), (of, IN),...   \n",
      "1              48  [[(``, ``), (it, PRP), (is, VBZ), (a, DT), (hu...   \n",
      "2              46  [[(as, IN), (she, PRP), (reached, VBD), (for, ...   \n",
      "3              60  [[(despite, IN), (the, DT), (scowl, NN), (that...   \n",
      "4              37  [[(the, DT), (traditional, JJ), (institutions,...   \n",
      "\n",
      "   cf_comma  ...     pf_VB    pf_VBD    pf_VBG    pf_VBN    pf_VBP    pf_VBZ  \\\n",
      "0  0.008080  ...  0.037383  0.046729  0.010681  0.018692  0.017356  0.029372   \n",
      "1  0.008789  ...  0.043042  0.101865  0.012912  0.002869  0.005739  0.014347   \n",
      "2  0.020848  ...  0.028494  0.071913  0.016282  0.014925  0.018996  0.005427   \n",
      "3  0.010016  ...  0.046414  0.064698  0.032349  0.015471  0.011252  0.015471   \n",
      "4  0.011827  ...  0.053812  0.061286  0.026906  0.014948  0.019432  0.005979   \n",
      "\n",
      "     pf_WDT     pf_WP    pf_WP$    pf_WRB  \n",
      "0  0.000000  0.005340  0.000000  0.010681  \n",
      "1  0.000000  0.008608  0.001435  0.007174  \n",
      "2  0.000000  0.001357  0.000000  0.012212  \n",
      "3  0.004219  0.005626  0.001406  0.004219  \n",
      "4  0.002990  0.001495  0.000000  0.004484  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "top_keys: ['NN', 'IN', 'PRP', 'VBD', 'DT', 'RB', 'JJ', 'VB', 'NNS', 'CC']\n",
      "NN: 44.975154166146794\n",
      "IN: 22.32512869820585\n",
      "PRP: 19.04376532042557\n",
      "VBD: 18.412767001595764\n",
      "DT: 17.76950693610078\n",
      "RB: 14.991828363870471\n",
      "JJ: 13.14492056301738\n",
      "VB: 10.011804957715917\n",
      "NNS: 7.756868460370113\n",
      "CC: 7.376109918937489\n"
     ]
    }
   ],
   "source": [
    "# create all the necessary features for the DEV set:\n",
    "dev_with_features,dev_feature_names,feature_groups = extract_features(DEV_SET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3205199c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# perform an ablation test on each type of model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_type, args_kwargs \u001b[38;5;129;01min\u001b[39;00m model_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 15\u001b[0m     \u001b[43mablation_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43margs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_with_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdev_feature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkwargs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 28\u001b[0m, in \u001b[0;36mablation_test\u001b[0;34m(model_type, data_frame, feature_names, *args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     prettified_feature_name \u001b[38;5;241m=\u001b[39m feature\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# train and test the model\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     scores[prettified_feature_name] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mablated_feature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[1;32m     39\u001b[0m bars \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mbar(scores\u001b[38;5;241m.\u001b[39mkeys(), scores\u001b[38;5;241m.\u001b[39mvalues())\n",
      "Cell \u001b[0;32mIn[72], line 35\u001b[0m, in \u001b[0;36mModel.train_and_score\u001b[0;34m(cls, X_train, y_train, X_test, y_test, feature_names, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_and_score\u001b[39m(\u001b[38;5;28mcls\u001b[39m, X_train, y_train, X_test, y_test, feature_names, \u001b[38;5;241m*\u001b[39margs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(X_train, y_train, feature_names, \u001b[38;5;241m*\u001b[39margs, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n",
      "Cell \u001b[0;32mIn[72], line 45\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m DecisionTreeClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1363\u001b[0m     )\n\u001b[1;32m   1364\u001b[0m ):\n\u001b[0;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:1024\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/tree/_classes.py:252\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    248\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    249\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    251\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 252\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2966\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[1;32m   2965\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[0;32m-> 2966\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[1;32m   2968\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:925\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    921\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt will be converted to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m         )\n\u001b[1;32m    924\u001b[0m dtypes_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[0;32m--> 925\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pandas_dtype_needs_early_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdtypes_orig\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    929\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:926\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    919\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    920\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame with sparse columns found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    921\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt will be converted to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m         )\n\u001b[1;32m    924\u001b[0m dtypes_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtypes)\n\u001b[1;32m    925\u001b[0m pandas_requires_conversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m--> 926\u001b[0m     \u001b[43m_pandas_dtype_needs_early_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m dtypes_orig\n\u001b[1;32m    927\u001b[0m )\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dtype_iter, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;28;01mfor\u001b[39;00m dtype_iter \u001b[38;5;129;01min\u001b[39;00m dtypes_orig):\n\u001b[1;32m    929\u001b[0m     dtype_orig \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresult_type(\u001b[38;5;241m*\u001b[39mdtypes_orig)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:700\u001b[0m, in \u001b[0;36m_pandas_dtype_needs_early_conversion\u001b[0;34m(pd_dtype)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    695\u001b[0m     is_bool_dtype,\n\u001b[1;32m    696\u001b[0m     is_float_dtype,\n\u001b[1;32m    697\u001b[0m     is_integer_dtype,\n\u001b[1;32m    698\u001b[0m )\n\u001b[0;32m--> 700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_bool_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd_dtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# bool and extension booleans need early conversion because __array__\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;66;03m# converts mixed dtype dataframes into object dtypes\u001b[39;00m\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pd_dtype, SparseDtype):\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# Sparse arrays will be converted later in `check_array`\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1252\u001b[0m, in \u001b[0;36mis_bool_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     arr_or_dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mcategories\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;66;03m# now we use the special definition for Index\u001b[39;00m\n\u001b[0;32m-> 1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marr_or_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mABCIndex\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;66;03m# Allow Index[object] that is all-bools or Index[\"boolean\"]\u001b[39;00m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr_or_dtype\u001b[38;5;241m.\u001b[39minferred_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboolean\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_bool_dtype(arr_or_dtype\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   1256\u001b[0m             \u001b[38;5;66;03m# GH#52680\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/dtypes/generic.py:42\u001b[0m, in \u001b[0;36mcreate_pandas_abc_type.<locals>._instancecheck\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(inst, attr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_typ\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m comp\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/1006\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# error: 'classmethod' used with a non-method\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_instancecheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _check(inst) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inst, \u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_subclasscheck\u001b[39m(\u001b[38;5;28mcls\u001b[39m, inst) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Raise instead of returning False\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# This is consistent with default __subclasscheck__ behavior\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dictionary with the selected model types and their optional arguments:\n",
    "model_dict = {\n",
    "    DecisionTree: {\n",
    "        'args': [],\n",
    "        'kwargs': {}\n",
    "    },\n",
    "    RandomForest:{\n",
    "        'args': [],\n",
    "        'kwargs': {'n_estimators': 100}\n",
    "    }\n",
    "}\n",
    "\n",
    "# perform an ablation test on each type of model\n",
    "for model_type, args_kwargs in model_dict.items():\n",
    "    ablation_test(\n",
    "        *args_kwargs[\"args\"],\n",
    "        model_type=model_type,\n",
    "        data_frame=dev_with_features,\n",
    "        feature_names=dev_feature_names,\n",
    "        **args_kwargs[\"kwargs\"]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177450d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6484c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937c4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
