{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41083305",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70bad916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.10.7)\n",
      "Requirement already satisfied: pandas in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.7.2)\n",
      "Requirement already satisfied: nltk in /home/hitomi/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.9.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (3.2.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.4.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (4.60.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/hitomi/.local/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/hitomi/.local/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/hitomi/.local/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/hitomi/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/hitomi/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/hitomi/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 4)) (1.15.3)\n",
      "Requirement already satisfied: click in /home/hitomi/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (8.3.0)\n",
      "Requirement already satisfied: tqdm in /home/hitomi/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/hitomi/.local/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 5)) (2025.10.23)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# install required libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd86ccb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package tagsets_json is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/hitomi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.data import find\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import MLEProbDist\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('tagsets_json')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')  # POS tagger model\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # POS tagger model\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.data import load\n",
    "tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "POS_TAGS = tagdict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc48471",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6529b126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "data_folder_name = 'A4data_2526_pan2020'\n",
    "DATA_FOLDER = Path(data_folder_name)\n",
    "DEV_SET = None\n",
    "TEST_SET = None\n",
    "TRAIN_SET = None\n",
    "\n",
    "if not DATA_FOLDER.exists():\n",
    "    print(f\"ERROR: Data folder not found, make sure the '/{data_folder_name}' folder is located in the same folder as this notebook!\")\n",
    "else:\n",
    "    try:\n",
    "        DEV_SET = pd.read_csv(DATA_FOLDER / 'pan2526_dev_data.csv')\n",
    "        TEST_SET = pd.read_csv(DATA_FOLDER / 'pan2526_test_data.csv')\n",
    "        TRAIN_SET = pd.read_csv(DATA_FOLDER / 'pan2526_train_data.csv')\n",
    "        print('Data loaded successfully')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Data not found in '/{DATA_FOLDER}'\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d5f85",
   "metadata": {},
   "source": [
    "Let's have a look at the DEV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4074360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3329</td>\n",
       "      <td>The last part of the pictorial will now begin!...</td>\n",
       "      <td>560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1225</td>\n",
       "      <td>\"It is a huge body of water.\" It was slow yet ...</td>\n",
       "      <td>560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3334</td>\n",
       "      <td>As she reached for the door, the sound of a ca...</td>\n",
       "      <td>512464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3573</td>\n",
       "      <td>Despite the scowl that seemed to be forever pl...</td>\n",
       "      <td>2750536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1429</td>\n",
       "      <td>The traditional institutions did not take the ...</td>\n",
       "      <td>1112924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1432</td>\n",
       "      <td>\"Still thinking about it, Kuwabara-jiisan,\" Hi...</td>\n",
       "      <td>1112924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>1838</td>\n",
       "      <td>\"Well, if it isn\"t tha\" police kitteh.\" Seras ...</td>\n",
       "      <td>1220273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>3328</td>\n",
       "      <td>\"I think I have seen that clothes before...\" L...</td>\n",
       "      <td>560480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>470</td>\n",
       "      <td>\"I almost didn\"t turn away from you, even when...</td>\n",
       "      <td>1220273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2037</td>\n",
       "      <td>\"No...\" \"Alright, so what you want to do is pu...</td>\n",
       "      <td>2855986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               text   author\n",
       "0          3329  The last part of the pictorial will now begin!...   560480\n",
       "1          1225  \"It is a huge body of water.\" It was slow yet ...   560480\n",
       "2          3334  As she reached for the door, the sound of a ca...   512464\n",
       "3          3573  Despite the scowl that seemed to be forever pl...  2750536\n",
       "4          1429  The traditional institutions did not take the ...  1112924\n",
       "..          ...                                                ...      ...\n",
       "263        1432  \"Still thinking about it, Kuwabara-jiisan,\" Hi...  1112924\n",
       "264        1838  \"Well, if it isn\"t tha\" police kitteh.\" Seras ...  1220273\n",
       "265        3328  \"I think I have seen that clothes before...\" L...   560480\n",
       "266         470  \"I almost didn\"t turn away from you, even when...  1220273\n",
       "267        2037  \"No...\" \"Alright, so what you want to do is pu...  2855986\n",
       "\n",
       "[268 rows x 3 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEV_SET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec4a7b8",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cb9c479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(sentences):\n",
    "    return [nltk.pos_tag(word_tokenize(sent)) for sent in sentences]\n",
    "\n",
    "def prepare(data_frame):\n",
    "    data_frame['snippet_length'] = data_frame['text'].str.len()\n",
    "    \n",
    "    data_frame['tokens'] = data_frame['text'].apply(\n",
    "        lambda x: word_tokenize(x.lower())\n",
    "    )\n",
    "    data_frame['token_count'] = data_frame['tokens'].apply(\n",
    "        lambda x: len(x)\n",
    "    )\n",
    "    data_frame['sentences'] = data_frame['text'].apply(\n",
    "        lambda x: sent_tokenize(x.lower())\n",
    "    )\n",
    "    data_frame['sentence_count'] = data_frame['sentences'].apply(\n",
    "        lambda x: len(x)\n",
    "    )\n",
    "    data_frame['pos_tagged_sentences'] = data_frame['sentences'].apply(\n",
    "        lambda x: pos_tagger(x)\n",
    "    )\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b24b32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_counter(pos_tagged_sentences, tag):\n",
    "    tag_sum = 0\n",
    "    for sentence in pos_tagged_sentences:\n",
    "        for token_tag_pair in sentence:\n",
    "            if token_tag_pair[1] == tag:\n",
    "                tag_sum += 1\n",
    "    return tag_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e9ba6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_features(data_frame):\n",
    "    chosen_tags = ['CC', 'CD', 'DT', 'EX', 'FW', \n",
    "'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS',\n",
    "'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG',\n",
    "'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "    \n",
    "    \n",
    "    assert 'pos_tagged_sentences' in data_frame.columns\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    for tag in chosen_tags:\n",
    "        feature_name = f'pf_{tag}'\n",
    "    \n",
    "        data_frame[feature_name] = data_frame['pos_tagged_sentences'].apply(\n",
    "            lambda x: tag_counter(x, tag)\n",
    "        ) / data_frame['token_count']\n",
    "\n",
    "    tag_sums = {}\n",
    "    for tag in chosen_tags:\n",
    "        feature_name = f'pf_{tag}'\n",
    "        tag_sums[tag] = data_frame[feature_name].sum()\n",
    "    \n",
    "    top_tags = sorted(tag_sums, key=tag_sums.get, reverse=True)[:10]\n",
    "    print(f'top_keys: {top_tags}')\n",
    "    for tag in top_tags:\n",
    "        feature_name = f'pf_{tag}'\n",
    "        feature_names.append(feature_name)\n",
    "        print(f'{tag}: {data_frame[feature_name].sum()}')\n",
    "    \n",
    "    return data_frame, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8b0bb3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(data_frame):\n",
    "#     data_frame['tokenized'] = data_frame['text'].apply(\n",
    "#         lambda x: word_tokenize(x.lower())\n",
    "#     )\n",
    "#     return data_frame\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf4b1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_features(data_frame):\n",
    "    # some char count fetaures\n",
    "    \n",
    "    data_frame['cf_comma'] = data_frame['text'].str.count(r',') / data_frame['snippet_length']\n",
    "    data_frame['cf_period'] = data_frame['text'].str.count(r'\\.') / data_frame['snippet_length']\n",
    "    data_frame['cf_exclam'] = data_frame['text'].str.count(r'\\!') / data_frame['snippet_length']\n",
    "    data_frame['cf_question'] = data_frame['text'].str.count(r'\\?') / data_frame['snippet_length']\n",
    "    data_frame['cf_upper_case'] = data_frame['text'].str.count(r'[A-Z]') / data_frame['snippet_length']\n",
    "    data_frame['cf_lower_case'] = data_frame['text'].str.count(r'[a-z]') / data_frame['snippet_length']\n",
    "    \n",
    "    # some aggregated char count features\n",
    "    data_frame['cf_vowel_frequency'] = data_frame['text'].str.count(r'[aeiou]') / data_frame['snippet_length']\n",
    "    data_frame['cf_avg_word_len'] = data_frame['text'].apply(lambda x: sum(len(w) for w in x.split()) / len(x.split()) if x.split() else 0)\n",
    "    \n",
    "    # make sure any features you want to be used during training are also in this list:\n",
    "    feature_names = [\n",
    "        'cf_comma', \n",
    "        'cf_period', \n",
    "        'cf_exclam', \n",
    "        'cf_question', \n",
    "        'cf_upper_case',\n",
    "        'cf_lower_case',\n",
    "        'cf_vowel_frequency',\n",
    "        'cf_avg_word_len'\n",
    "    ]\n",
    "    \n",
    "    return data_frame, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1d2c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_features(data_frame):\n",
    "    # count number of words\n",
    "    data_frame['wf_word_count'] = data_frame['text'].apply(\n",
    "        lambda x: len(x.split())\n",
    "    )\n",
    "    # count the stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    data_frame['wf_stopword_frequency'] = data_frame['text'].apply(\n",
    "        lambda x: sum(1 for w in word_tokenize(x.lower()) if w in stop_words)\n",
    "    ) / data_frame['wf_word_count']\n",
    "    \n",
    "    # make sure any features you want to be used during training are also in this list:\n",
    "    feature_names = [\n",
    "        'wf_word_count', \n",
    "        'wf_stopword_frequency'\n",
    "    ]\n",
    "    \n",
    "    return data_frame, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "02fca92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bow_features(data_frame, vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        #normalizing the score and extracting only the top 10 words\n",
    "        vectorizer = TfidfVectorizer(max_features=10)\n",
    "        matrix = vectorizer.fit_transform(data_frame['text'])\n",
    "    else:\n",
    "        matrix = vectorizer.transform(data_frame['text'])\n",
    "    feature_names = [\n",
    "        f'Tfidf_{w}' for w in vectorizer.get_feature_names_out()\n",
    "    ]\n",
    "    bow_df = pd.DataFrame(matrix.toarray(), columns=feature_names, index=data_frame.index)\n",
    "    data_frame = pd.concat([data_frame, bow_df], axis=1)\n",
    "    return data_frame, feature_names, vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "168c3582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing pairs consisting only stopwords\n",
    "def remove_stopwords_pairs(ngram):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    removed = []\n",
    "    for pair in ngram:\n",
    "        count = 0\n",
    "        for word in pair:\n",
    "            if word not in stop_words:\n",
    "                count = 1\n",
    "                break\n",
    "        if (count==1):\n",
    "            removed.append(pair)\n",
    "    return removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6e02698c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_topnth_prob(data_frame, n):\n",
    "    topn_list = []\n",
    "    for token in data_frame['tokens']:\n",
    "        bigram_list = list(bigrams(token))\n",
    "        bigram_stopwords_removed_list = remove_stopwords_pairs(bigram_list)\n",
    "    \n",
    "        if not bigram_stopwords_removed_list:\n",
    "            topn_list.append({})\n",
    "            continue\n",
    "\n",
    "        freq_dist = FreqDist(bigram_stopwords_removed_list)\n",
    "        prob_dist = MLEProbDist(freq_dist)\n",
    "\n",
    "        bigram_prob_dict = {bg: prob_dist.prob(bg) for bg in freq_dist.keys()}\n",
    "\n",
    "        sorted_dict = dict(sorted(bigram_prob_dict.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "        topn_list.append(sorted_dict)\n",
    "\n",
    "    \n",
    "    bigram_prob_df = pd.DataFrame(topn_list).fillna(0)\n",
    "    bigram_prob_df.columns = [f'bigram_{bg}' for bg in bigram_prob_df.columns]\n",
    "\n",
    "    data_frame = pd.concat([data_frame.reset_index(drop=True), bigram_prob_df.reset_index(drop=True)], axis=1)\n",
    "    feature_names = list(bigram_prob_df.columns)\n",
    "    return data_frame, feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e8be30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_frame):\n",
    "    \"\"\"extracts relevant features\"\"\"\n",
    "    # prepare text (like tokenizing)\n",
    "    print(data_frame.head())\n",
    "    # list of feature names we want to use for training\n",
    "    feature_names = []\n",
    "    feature_groups = {}\n",
    "    \n",
    "    # extract some character based features\n",
    "    data_frame, ft_names = get_char_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"char\"] = ft_names\n",
    "    # extract stop word based features\n",
    "    data_frame, ft_names = get_word_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"stopword_frequency\"] = ft_names\n",
    "    \n",
    "    # extract POS features\n",
    "    data_frame, ft_names = get_pos_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"pos_tags\"] = ft_names\n",
    "\n",
    "    #extract bag of words feature using the top 10 words\n",
    "    data_frame, ft_names, vectorizer = get_bow_features(data_frame)\n",
    "    feature_names += ft_names\n",
    "    feature_groups[\"Tfidf\"] = ft_names\n",
    "\n",
    "    # return the data with the features and the list of feature names\n",
    "    return data_frame, feature_names, feature_groups, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fd813084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"\n",
    "    A wrapper class that is meant to wrap around an arbitrary machine learning model\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train, y_train, feature_names, *args, verbose=False, **kwargs):\n",
    "        self.args=args\n",
    "        self.kwargs=kwargs\n",
    "        self.X_train=X_train[feature_names]\n",
    "        self.y_train=y_train\n",
    "        self.feature_names=feature_names\n",
    "        self.verbose=verbose\n",
    "    \n",
    "    def fit(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _test(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _score_fn(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def predict(self, X_test, y_test):\n",
    "        self.X_test=X_test[self.feature_names]\n",
    "        self.y_test=y_test\n",
    "        return self._test()\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        self.X_test=X_test[self.feature_names]\n",
    "        self.y_test=y_test\n",
    "        return self._score_fn()\n",
    "    \n",
    "    @classmethod\n",
    "    def train_and_score(cls, X_train, y_train, X_test, y_test, feature_names, *args, verbose=False, **kwargs):\n",
    "        model = cls(X_train, y_train, feature_names, *args, verbose=verbose, **kwargs)\n",
    "        model.fit()\n",
    "        return model.score(X_test, y_test)\n",
    "    \n",
    "\n",
    "class DecisionTree(Model):\n",
    "    \"\"\"\n",
    "    Wrapper for the sklearn DecisionTree\n",
    "    \"\"\"\n",
    "    def fit(self):\n",
    "        self.model = DecisionTreeClassifier(*self.args, **self.kwargs)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def _test(self):\n",
    "        # Make predictions\n",
    "        return self.model.predict(self.X_test)\n",
    "    \n",
    "    def _score_fn(self):\n",
    "        return self.model.score(self.X_test, self.y_test)\n",
    "    \n",
    "class RandomForest(Model):\n",
    "    \"\"\"\n",
    "    Wrapper for the sklearn RandomForest\n",
    "    \"\"\"\n",
    "    def fit(self):\n",
    "        self.model = RandomForestClassifier(*self.args, **self.kwargs)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def _test(self):\n",
    "        # Make predictions for the test set\n",
    "        return self.model.predict(self.X_test)\n",
    "    \n",
    "    def _score_fn(self):\n",
    "        # Get the accuracy on the test set\n",
    "        return self.model.score(self.X_test, self.y_test)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0c2471e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_test(model_type:Model, X_train, X_test, y_train, y_test, feature_names, feature_groups, ax, is_last, *args, **kwargs):\n",
    "\n",
    "    scores = {}\n",
    "    colors = []\n",
    "    alphas = []\n",
    "    # get the base score when using all features:\n",
    "    scores['None'] = model_type.train_and_score(\n",
    "        *args,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        feature_names=feature_names,\n",
    "        **kwargs\n",
    "    )\n",
    "    colors.append('C0')\n",
    "    alphas.append(1)\n",
    "    # perform the actual ablation tests:\n",
    "    for i, feature_group in enumerate(feature_groups):\n",
    "        prettified_feature_group_name = f'all {feature_group.replace(\"_\", \" \") }'\n",
    "        group_features = feature_groups[feature_group]\n",
    "        # remove all group features\n",
    "        ablated_feature_names = feature_names.copy() \n",
    "        for feature in group_features:\n",
    "            ablated_feature_names.remove(feature)  \n",
    "        \n",
    "        # train and test the model\n",
    "         \n",
    "        scores[prettified_feature_group_name] = model_type.train_and_score(\n",
    "            *args,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            X_test=X_test,\n",
    "            y_test=y_test,\n",
    "            feature_names=ablated_feature_names,\n",
    "            **kwargs\n",
    "        )\n",
    "        colors.append(f'C{i+1}')\n",
    "        alphas.append(1)\n",
    "    \n",
    "        \n",
    "    \n",
    "    bars = ax.bar(scores.keys(), scores.values())\n",
    "    for i, bar in enumerate(bars):\n",
    "        bar.set_color(colors[i])\n",
    "        bar.set_alpha(alphas[i])\n",
    "    #bars[0].set_color('C1')\n",
    "    if is_last:\n",
    "        for i, label in enumerate(ax.get_xticklabels()):\n",
    "            label.set_rotation(45)\n",
    "            label.set_ha('right')\n",
    "            if alphas[i] == 1:\n",
    "                label.set_fontweight('bold')\n",
    "        ax.set_xlabel('Ablated feature')\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    \n",
    "    ax.set_title(f'{model_type.__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3205199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_all(\n",
    "    X_train,\n",
    "    X_test, \n",
    "    y_train,\n",
    "    y_test,\n",
    "    feature_names,\n",
    "    feature_groups\n",
    "):# dictionary with the selected model types and their optional arguments:\n",
    "\n",
    "    \n",
    "    model_dict = {\n",
    "        DecisionTree: {\n",
    "            'args': [],\n",
    "            'kwargs': {}\n",
    "        },\n",
    "        RandomForest:{\n",
    "            'args': [],\n",
    "            'kwargs': {'n_estimators': 100}\n",
    "        }\n",
    "    }\n",
    "    n_models=len(model_dict)\n",
    "    fig, axs = plt.subplots(n_models,1, figsize=(12,3*n_models))\n",
    "    print(len(axs))\n",
    "    print(f'Training and testing {n_models} models')\n",
    "    # perform an ablation test on each type of model\n",
    "    for i, (model_type, args_kwargs) in enumerate(model_dict.items()):\n",
    "\n",
    "        print(f'Model {model_type.__name__}:')\n",
    "        ablation_test(\n",
    "            *args_kwargs[\"args\"],\n",
    "            model_type=model_type,\n",
    "            X_train=X_train,\n",
    "            X_test=X_test,\n",
    "            y_train=y_train,\n",
    "            y_test=y_test,\n",
    "            feature_names=feature_names,\n",
    "            feature_groups=feature_groups,\n",
    "            ax=axs[i],\n",
    "            is_last=i==n_models-1,\n",
    "            **args_kwargs[\"kwargs\"]\n",
    "        )\n",
    "    plt.suptitle('Ablation Results')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5177450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_dev():\n",
    "    dev_prepared_df = prepare(DEV_SET)\n",
    "    dev_with_bigrams_df, dev_bigram_features = get_bigram_topnth_prob(dev_prepared_df, n=50)\n",
    "    dev_with_features,dev_feature_names,dev_feature_groups, vectorizer = extract_features(dev_with_bigrams_df)\n",
    "    dev_feature_names += dev_bigram_features\n",
    "    dev_feature_groups[\"bigram\"] = dev_bigram_features\n",
    "    X_dev = dev_with_features[dev_feature_names]\n",
    "    y_dev = dev_with_features['author']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_dev, y_dev, test_size=0.2, random_state=42)\n",
    "    train_and_test_all(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        feature_names=dev_feature_names,\n",
    "        feature_groups=dev_feature_groups\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad405ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_train_test():\n",
    "    train_prepared_df = prepare(TRAIN_SET)\n",
    "    train_with_bigrams_df, train_bigram_features = get_bigram_topnth_prob(train_prepared_df, n=10)\n",
    "    train_with_features, train_feature_names, train_feature_groups, train_vectorizer = extract_features(train_with_bigrams_df)\n",
    "    train_feature_names += train_bigram_features\n",
    "    train_feature_groups[\"bigram\"] = train_bigram_features\n",
    "    X_train = train_with_features[train_feature_names]\n",
    "    y_train = train_with_features['author']\n",
    "    test_with_features, test_feature_names, test_feature_groups, _ = extract_features(TEST_SET, vectorizer=train_vectorizer)\n",
    "    X_test = test_with_features[test_feature_names]\n",
    "    y_test = test_with_features['author']\n",
    "    train_and_test_all(\n",
    "        X_train=X_train,\n",
    "        X_test=X_test,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        feature_names=train_feature_names,\n",
    "        feature_groups=train_feature_groups\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e9c6484c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_on_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6937c4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
